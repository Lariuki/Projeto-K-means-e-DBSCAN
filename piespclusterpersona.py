# -*- coding: utf-8 -*-
"""PiespClusterPersona.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1znd-ZHW2B3JUhO5xDWg9nmVRJ8sVUcrA

Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp)
(https://www.piesp.seade.gov.br/ )
![PIESP.PNG](attachment:PIESP.PNG)
Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp)
A PIESP, realizada pela Fundação Seade desde 1998 e, desde o início de 2012, com metodologia revisada, acompanha os anúncios de investimentos produtivos de empresas veiculados na imprensa, fornecendo informações que auxiliam a identificação de tendências setoriais e regionais da economia paulista.

******: Augusto Camargos

***Date:*** Oct/2020

***Version***: 1.0

<center>
<img src='C:/Users/gesta/TERA/InvestSP/PIESP.PNG alt="PIESP">
</center>

(Página Inicial > Pesquisas em Campo >  Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp) )
A PIESP, realizada pela Fundação Seade desde 1998 e, desde o início de 2012, com metodologia revisada, acompanha os anúncios de investimentos produtivos de empresas veiculados na imprensa, fornecendo informações que auxiliam a identificação de tendências setoriais e regionais da economia paulista.

Todos os anúncios são confirmados com os investidores, por telefone.
"""

import pandas as pd

import warnings
warnings.filterwarnings('ignore')

#to change the scientific notation precision
pd.set_option('display.float_format', lambda x: '%.2f' % x)

"""## Recriando o Dataframe dfInvestSPVal que passou pelo processo de EDA e datacleansing no notebook PiespEDA e foi armazenado no arquivo dfInvestVal.csv"""

#dfInvestVal.to_csv('dfInvesVal.csv')
dfInvestVal = pd.read_csv('dfInvestVal.csv')
dfInvestVal.head()

dfInvestVal.info()

dfInvestVal.shape

"""## Estruturando processo de Clusters
Criando as funções que serão utilizadas na sequência
"""

# https://deeplearningcourses.com/c/cluster-analysis-unsupervised-machine-learning-python
# https://www.udemy.com/cluster-analysis-unsupervised-machine-learning-python
from __future__ import print_function, division
from future.utils import iteritems
from builtins import range, input
from sklearn.cluster import KMeans
# Note: you may need to update your version of future
# sudo pip install -U future
#!pip install -U future

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import pairwise_distances

# Augusto - função custo/objetivo que precisa ser minimizada para termos o ponto ótimo do algoritmo
def cost(X, R, M):
    cost = 0
    for k in range(len(M)):
        # method 1
        # for n in range(len(X)):
        #     cost += R[n,k]*d(M[k], X[n])

        # method 2
        diff = X - M[k]
        sq_distances = (diff * diff).sum(axis=1)
        cost += (R[:,k] * sq_distances).sum()
    return cost

# Augusto - apenas para fazer a diferença de matrizes
def d(u, v):
    diff = u - v
    return diff.dot(diff)

# Augusto - função principal que vai classificar os elementos nos clusters e plotar o gráfico separando os cluster por cores
def plot_k_means(X, K, max_iter=20, beta=3.0, show_plots=False):
    N, D = X.shape
    # R = np.zeros((N, K))
    exponents = np.empty((N, K))

    # initialize M to random
    initial_centers = np.random.choice(N, K, replace=False)
    M = X[initial_centers]

    costs = []
    k = 0
    #Augusto =  nos loops abaixo é implementado a teoria / algoritmo k-means
    #Augusto =  k é a quantidade de clusters e means é porque é utilizado a média mais próxima dos núcleos para
    # definir à qual cluster o item pertence

    for i in range(max_iter):
        k += 1
        # step 1: determine assignments / resposibilities
        # is this inefficient?
        # Augusto = step1 - atribuir as responsabilidades
        for k in range(K):
            for n in range(N):
                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))
        R = exponents / exponents.sum(axis=1, keepdims=True)


        # step 2: recalculate means
        # Augusto - step 2  - processo análogo ao gradiente descendente , porém aqui é chamado de
        # decent vectorization
        # for k in range(K):
        #     M[k] = R[:,k].dot(X) / R[:,k].sum()
        # oldM = M

        # full vectorization
        M = R.T.dot(X) / R.sum(axis=0, keepdims=True).T
        # print("diff M:", np.abs(M - oldM).sum())

        c = cost(X, R, M)
        costs.append(c)
        if i > 0:
            if np.abs(costs[-1] - costs[-2]) < 1e-5:
                break

        if len(costs) > 1:
            if costs[-1] > costs[-2]:
                pass
                # print("cost increased!")
                # print("M:", M)
                # print("R.min:", R.min(), "R.max:", R.max())

    if show_plots:
        plt.plot(costs)
        plt.title("Costs")
        plt.show()

        random_colors = np.random.random((K, 3))
        #.dot  - multiplicação de matrizes
        colors = R.dot(random_colors)
        plt.scatter(X[:,0], X[:,1], c=colors)
        plt.show()

    print("Final cost", costs[-1])
    return M, R

#Função para transformar o setor em valor numérico, pois os algoritmos de Cluster trabalham apenas com dimensões numéricas
def transf_regiao():
    tam = len(dfInvestVal2)
    mu, sigma = 0, 0.1 # mean and standard deviation
    fator = np.random.normal(mu, sigma, tam)
    n=0
    for n in range(tam):
        if dfInvestVal2['Região'][n] == 'RM São Paulo':
            dfInvestVal2['RegiãoNro'][n] = 1  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'Inter-regionais':
            dfInvestVal2['RegiãoNro'][n] = 3   #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Campinas':
            dfInvestVal2['RegiãoNro'][n] = 5  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Santos':
            dfInvestVal2['RegiãoNro'][n] = 7  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA São José dos Campos':
            dfInvestVal2['RegiãoNro'][n] = 9  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Bauru':
            dfInvestVal2['RegiãoNro'][n] = 11  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Sorocaba':
            dfInvestVal2['RegiãoNro'][n] = 13  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Central':
            dfInvestVal2['RegiãoNro'][n] = 15  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA São José do Rio Preto':
            dfInvestVal2['RegiãoNro'][n] = 17  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Araçatuba':
            dfInvestVal2['RegiãoNro'][n] = 19  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Ribeirão Preto':
            dfInvestVal2['RegiãoNro'][n] = 21  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Ribeirão Preto':
            dfInvestVal2['RegiãoNro'][n] = 23  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Barretos':
            dfInvestVal2['RegiãoNro'][n] = 25  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Ribeirão Preto':
            dfInvestVal2['RegiãoNro'][n] = 27  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Marília':
            dfInvestVal2['RegiãoNro'][n] = 29  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Franca':
            dfInvestVal2['RegiãoNro'][n] = 31  #+ fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Presidente Prudente':
            dfInvestVal2['RegiãoNro'][n] = 33  # + fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Itapeva':
            dfInvestVal2['RegiãoNro'][n] = 35  #  + fator[n]
        elif dfInvestVal2['Região'][n] == 'RA Registro':
            dfInvestVal2['RegiãoNro'][n] = 37   #+ fator[n]
        else: dfnvestVal2['RegiãoNro'][n] = 555
        n += 1
    return dfInvestVal2

#Função para transformar o setor em valor numérico, pois os algoritmos de Cluster trabalham apenas com dimensões numéricas
def transf_setor():
    tam = len(dfInvestVal)
    dfInvestVal2 = dfInvestVal
    n=0
    for n in range(tam):
        if dfInvestVal2['Setor'][n] == 'Comércio':
            dfInvestVal2['Setor'][n] = 1
        elif dfInvestVal2['Setor'][n] == 'Indústria':
            dfInvestVal2['Setor'][n] = 3
        elif dfInvestVal2['Setor'][n] == 'Infraestrutura':
            dfInvestVal2['Setor'][n] = 5
        elif dfInvestVal2['Setor'][n] == 'Serviços':
            dfInvestVal2['Setor'][n] = 7
        elif dfInvestVal2['Setor'][n] == 'Outros':
            dfInvestVal2['Setor'][n] = 9
        n += 1
    return dfInvestVal2

# Função para transformar o setor de cada item agregando valores aleatórios entre zero e um conforme distribuição normal através da função choice da biblioteca numpy
# essa função é na realidade mais para dar um efeito estético de cluster, apresentando os pontos como nuvem, senão ficaraim linhas/tripas
def transf_setor2():
    tam = len(dfInvestVal2)
    n=0
    fator = np.random.choice(3, tam)
    i = 0.0001
    for n in range(tam):
        if dfInvestVal2['Setor'][n] == 1:
            dfInvestVal2['Setor'][n] = (1) + fator[n]
        elif dfInvestVal2['Setor'][n] == 7:
            dfInvestVal2['Setor'][n] = (7) + fator[n]
        elif dfInvestVal2['Setor'][n] == 13:
            dfInvestVal2['Setor'][n] = (13) + fator[n]
        elif dfInvestVal2['Setor'][n] == 19:
            dfInvestVal2['Setor'][n] = (19) + fator[n]
        elif dfInvestVal2['Setor'][n] == 25:
            dfInvestVal2['Setor'][n] = (25) + fator[n]
        n += 1
    return dfInvestVal2

## Salvando o Nome do Setor e Tipo de Atividade em colunas relacionadas para preservar o texto original
dfInvestVal['SetorNome'] = dfInvestVal['Setor']
#dfInvestVal['Tipo Investimento Nome']=dfInvestVal['Tipo Investimento']

# Para refazer o Setor - reexecute a partir daqui
dfInvestVal2=transf_setor()

dfInvestVal2['Setor'] = dfInvestVal2['Setor'].astype(int)

#dfInvestVal2=transf_setor2()

dfInvestVal2['RegiãoNro'] = 0

transf_regiao()

pd.Series(dfInvestVal2[['Região','RegiãoNro']].value_counts())

#!pip install matplotlib
#!pip install sklearn.preprocessing
#!pip install -U scikit-learn
#!pip install -U matplotlib

dfInvestVal2.head()

"""# Criamos faixa de valor (P,M,G,GG)  e criamos o campo regiãoNro que é um de/para da região para um número correspondente.
# Pensando no conceito de Persona, criaremos clusters considerando faixa de valor ao invés de valor + região + setor
# Dessa forma, esperamos gerar clusters pensando que fornecedores e consultorias possa se beneficiar mais na sua busca por novos clientes.
# Por exemplo se a consultoria for uma pequena empresa de software de gestão que atue na região de Campinas com foco em empresas de serviços e se sente mais competitiva em projetos não tão grandes, poderia se interessar pelo Cluster de Investimentos de Faixa de Investimento = P + Região = Campinas + Setor= Serviços

## Criando Clusters com a lib skit learn - método K-Means
"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

"""# Standardize the data - Normalização/padronização dos Dados
Transformação dos dados - MinMaxScaler - fit_transform
https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/ Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.

The two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.

# Criando o Dataframe vet_x somente das colunas a serem clusterizadas
"""

from sklearn import metrics
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

#x_scaled - VETOR NORMALIZADO
#kmeans = KMeans(n_clusters = 5, random_state=123)
#klabels = kmeans.labels_

#print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(x_scaled, klabels))

"""# Criando os Clusters com 3 dimensões: Faixa de Valor, Setor e Região

# Criando a matriz vet_x2
"""

len(dfInvestVal2[['Setor','FaixaValor','RegiãoNro']])

vet_x2 = dfInvestVal2[['Setor','FaixaValor','RegiãoNro']]
vet_x2.head()

x_array2 = np.array(vet_x2)
print(x_array2)

"""## Standardize the data - Base: x_array2
## Transformação dos dados - MinMaxScaler - fit_transform
https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/
Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.

The two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.
"""

scaler3D = MinMaxScaler()
x_scaled3D = scaler3D.fit_transform(x_array2)
print(x_scaled3D)
X3D = x_scaled3D
df_scaleMinMax = pd.DataFrame(x_scaled3D, columns=vet_x2.columns)
df_scaleMinMax.head()

#!pip install --upgrade matplotlib
df_scaleMinMax.head()

df_scaleMinMax.shape

# Import libraries
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

# Creating dataset
xMinMax = X3D[:,0]
yMinMax = X3D[:,1]
zMinMax = X3D[:,2]

"""# Standardize the data
# Transformação dos dados - StandardScaler - fit_transform
"""

scaler = StandardScaler()
scaler.fit(x_array2)
X_scale = scaler.transform(x_array2)
df_scaleStd = pd.DataFrame(X_scale, columns=vet_x2.columns)
# Creating dataset
xStd = X_scale[:,0]
yStd = X_scale[:,1]
zStd = X_scale[:,2]
df_scaleStd.head()





"""# Plotando os dados "crus" - transformados em 3D"""

from matplotlib import cm
# Creating figyre
fig = plt.figure(figsize =(14, 9))
ax = plt.axes(projection ='3d')

# Select the color map named rainbow
cmap = cm.get_cmap(name='rainbow')

# Creating plot
#ax.plot_surface(x, y, X2[:,2])
#ax.scatter(x, y, z,c = 'red')
#ax.plot(x, y, z)
#ax.plot_trisurf(x, y, z)
#plot_trisurf(x,y, triangles=triangles,z)
ax.plot_trisurf(xMinMax, yMinMax, zMinMax, linewidth=0.2, antialiased=True)

# Plot each vector with a different color from the colormap.
#for ind, (t, y, v) in enumerate(zip(timeDate, Y, V)):
#|    plt.vlines(t,y,v ,color = cmap(ind))

# show plot
plt.show()

# Gráfico com recurso de malha - mash

#Two additional examples of plotting surfaces with triangular mesh.

#The first demonstrates use of plot_trisurf's triangles argument, and the
#second sets a Triangulation object's mask and passes the object directly
#to plot_trisurf.
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.tri as mtri


fig = plt.figure(figsize=plt.figaspect(0.5))

#============
# First plot
#============

# Make a mesh in the space of parameterisation variables u and v
u = np.linspace(0, 2.0 * np.pi, endpoint=True, num=50)
v = np.linspace(-0.5, 0.5, endpoint=True, num=10)
u, v = np.meshgrid(u, v)
u, v = u.flatten(), v.flatten()

# This is the Mobius mapping, taking a u, v pair and returning an x, y, z
# triple

# Triangulate parameter space to determine the triangles
tri = mtri.Triangulation(u, v)

# Plot the surface.  The triangles in parameter space determine which x, y, z
# points are connected by an edge.
ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_trisurf(xMinMax, yMinMax, zMinMax, triangles=tri.triangles, cmap=plt.cm.Spectral)
ax.set_zlim(-1, 1)

"""# Plotting our raw data in a 3D space we can see some potential problems for Clustering"""

import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Faixa Valor'),zaxis = dict(title  = 'Região'))
trace = go.Scatter3d(x=xMinMax, y=yMinMax, z=zMinMax, mode='markers',marker=dict(colorscale='Greys', opacity=0.3, size = 10, ))
#trace = go.Scatter3d(x=df_scale.iloc[:,0], y=df_scale.iloc[:,1], z=df_scale[:,2], mode='markers',marker=dict(colorscale='Greys', opacity=0.3, size = 10, ))
layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.show()

"""# Criando Clusters com a lib skit learn - método K-Means
https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6

"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go
from plotly import tools
from plotly.subplots import make_subplots
import plotly.offline as py
import plotly.express as px
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
# %matplotlib inline
from warnings import filterwarnings
filterwarnings('ignore')

"""2. KMeans on Original Dataset
Let’s utilize the Elbow Method to determine the optimal number of clusters KMeans should obtain. It seem 4 or 5 clusters would be best and for the sake of simplicity we’ll select 5.
"""

df_scale3 = df_scaleMinMax.copy()
sse = []
k_list = range(1, 15)
for k in k_list:
    km = KMeans(n_clusters=k)
    km.fit(df_scale3)
    sse.append([k, km.inertia_])

oca_results_scale = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})
plt.figure(figsize=(12,6))
plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o')
plt.title('Optimal Number of Clusters using Elbow Method (Scaled Data)')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

"""Veja que o k-means não parece ter bom desempenho para 3 dimensões, pois gerou apenas 4 cluster como número ótimo de clusters, o que claramente é muito aquém pela inspeção visual dos dados "crus". Mais abaixo temos a comprovação dessa ineficiência com o valor do teste de silhueta com apenas 0.53 (muito baixo se comparado com o valor da silhueta do DBSCAN que veremos abaixo com quase 0.9! Isto pelo k=means não houve uma boa segregação dos clusters."""

kmeans_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(df_scale3)
dfInvestVal2['klusterK3D'] = kmeans_scale.labels_
print('KMeans Scaled Silhouette Score: {}'.format(silhouette_score(df_scale3, kmeans_scale.labels_, metric='euclidean')))
labels_scale = kmeans_scale.labels_
clusters_scale = pd.concat([df_scale3, pd.DataFrame({'cluster_scaled':labels_scale})], axis=1)

kmeans_scale.labels_

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Faixa Valor'),zaxis = dict(title  = 'Região'))
labels = labels_scale
trace = go.Scatter3d(x=xMinMax, y=yMinMax, z=zMinMax, mode='markers',marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))
layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.show()

"""#  Cluster 3D with DBSCAN
https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31
"""

#!pip install plotly

#Let’s Cluster!
import itertools
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go
from plotly import tools
from plotly.subplots import make_subplots
import plotly.offline as py
import plotly.express as px
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
plt.style.use('fivethirtyeight')
from warnings import filterwarnings
filterwarnings('ignore')

from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from collections import Counter

"""## DBSCAN Clustering
Approach #1
Before we apply the clustering algorithm we have to determine the appropriate epsilon level using the “Elbow Method” we discussed above. It would seem the optimal epsilon value is around 0.1. Finally, since we have 3 principal components to our data we’ll set our minimum points criteria to 6.
"""

plt.figure(figsize=(10,5))
nn = NearestNeighbors(n_neighbors=5).fit(df_scaleStd)
distances, idx = nn.kneighbors(df_scaleStd)
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)
plt.show()

db = DBSCAN(eps=0.3, min_samples=4).fit(df_scaleStd)
labelsDB3D = db.labels_
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labelsDB3D)) - (1 if -1 in labelsDB3D else 0)
n_noise_ = list(labelsDB3D).count(-1)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(df_scaleStd, labelsDB3D))

dfInvestVal2.shape

pd.Series(labelsDB3D).unique()

dfInvestVal2['klusterBD3D'] = labelsDB3D

dfInvestVal2.head()

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Faixa de Valor'),zaxis = dict(title  = 'Região'))
#labels = db.labels_
trace = go.Scatter3d(x=xStd, y=yStd, z=zStd, mode='markers',marker=dict(color = labelsDB3D, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))
layout = go.Layout(scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.update_layout(title='DBSCAN clusters (46) ', font=dict(size=12,))
fig.show()

np.unique(labelsDB3D, return_counts=True)

# o DBSCAN têm uma certa dificuldade em identificar cluster quando tem diferentes densidades e/ou quando estão muito espalhados uns dos outros, que é o caso do maior cluster mais aéreo/ valores maiores de investimento no eixo Z

dfInvestVal2.head()

dfInvestVal2['klusterK3D'].unique()

dfInvestVal2['klusterBD3D'].unique()

dfInvestVal2.to_csv('dfClustersFinal2.csv')



