# -*- coding: utf-8 -*-
"""PiespCluster(Final)v3c.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pc7UKUAP9xuCNkq2d4VK6kd9KcuDM46h

Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp)
(https://www.piesp.seade.gov.br/ )
![PIESP.PNG](attachment:PIESP.PNG)
Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp)
A PIESP, realizada pela Fundação Seade desde 1998 e, desde o início de 2012, com metodologia revisada, acompanha os anúncios de investimentos produtivos de empresas veiculados na imprensa, fornecendo informações que auxiliam a identificação de tendências setoriais e regionais da economia paulista.

******: Augusto Camargos

***Date:*** Oct/2020

***Version***: 1.0

<center>
<img src='C:/Users/gesta/TERA/InvestSP/PIESP.PNG alt="PIESP">
</center>

(Página Inicial > Pesquisas em Campo >  Pesquisa de Investimentos Anunciados no Estado de São Paulo (Piesp) )
A PIESP, realizada pela Fundação Seade desde 1998 e, desde o início de 2012, com metodologia revisada, acompanha os anúncios de investimentos produtivos de empresas veiculados na imprensa, fornecendo informações que auxiliam a identificação de tendências setoriais e regionais da economia paulista.

Todos os anúncios são confirmados com os investidores, por telefone.
"""

import pandas as pd

import warnings
warnings.filterwarnings('ignore')

#to change the scientific notation precision
pd.set_option('display.float_format', lambda x: '%.2f' % x)

"""## Recriando o Dataframe dfInvestSPVal que passou pelo processo de EDA e datacleansing no notebook PiespEDA e foi armazenado no arquivo dfInvestVal.csv"""

#dfInvestVal.to_csv('dfInvesVal.csv')
dfInvestVal = pd.read_csv('dfInvestVal.csv')
dfInvestVal.head()

dfInvestVal.info()

dfInvestVal.shape

"""## Estruturando processo de Clusters
Criando as funções que serão utilizadas na sequência
"""

# https://deeplearningcourses.com/c/cluster-analysis-unsupervised-machine-learning-python
# https://www.udemy.com/cluster-analysis-unsupervised-machine-learning-python
from __future__ import print_function, division
from future.utils import iteritems
from builtins import range, input
# Note: you may need to update your version of future
# sudo pip install -U future
#!pip install -U future

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import pairwise_distances

# Augusto - função custo/objetivo que precisa ser minimizada para termos o ponto ótimo do algoritmo
def cost(X, R, M):
    cost = 0
    for k in range(len(M)):
        # method 1
        # for n in range(len(X)):
        #     cost += R[n,k]*d(M[k], X[n])

        # method 2
        diff = X - M[k]
        sq_distances = (diff * diff).sum(axis=1)
        cost += (R[:,k] * sq_distances).sum()
    return cost

# Augusto - apenas para fazer a diferença de matrizes
def d(u, v):
    diff = u - v
    return diff.dot(diff)

# Augusto - função principal que vai classificar os elementos nos clusters e plotar o gráfico separando os cluster por cores
def plot_k_means(X, K, max_iter=20, beta=3.0, show_plots=False):
    N, D = X.shape
    # R = np.zeros((N, K))
    exponents = np.empty((N, K))

    # initialize M to random
    initial_centers = np.random.choice(N, K, replace=False)
    M = X[initial_centers]

    costs = []
    k = 0
    #Augusto =  nos loops abaixo é implementado a teoria / algoritmo k-means
    #Augusto =  k é a quantidade de clusters e means é porque é utilizado a média mais próxima dos núcleos para
    # definir à qual cluster o item pertence

    for i in range(max_iter):
        k += 1
        # step 1: determine assignments / resposibilities
        # is this inefficient?
        # Augusto = step1 - atribuir as responsabilidades
        for k in range(K):
            for n in range(N):
                exponents[n,k] = np.exp(-beta*d(M[k], X[n]))
        R = exponents / exponents.sum(axis=1, keepdims=True)


        # step 2: recalculate means
        # Augusto - step 2  - processo análogo ao gradiente descendente , porém aqui é chamado de
        # decent vectorization
        # for k in range(K):
        #     M[k] = R[:,k].dot(X) / R[:,k].sum()
        # oldM = M

        # full vectorization
        M = R.T.dot(X) / R.sum(axis=0, keepdims=True).T
        # print("diff M:", np.abs(M - oldM).sum())

        c = cost(X, R, M)
        costs.append(c)
        if i > 0:
            if np.abs(costs[-1] - costs[-2]) < 1e-5:
                break

        if len(costs) > 1:
            if costs[-1] > costs[-2]:
                pass
                # print("cost increased!")
                # print("M:", M)
                # print("R.min:", R.min(), "R.max:", R.max())

    if show_plots:
        plt.plot(costs)
        plt.title("Costs")
        plt.show()

        random_colors = np.random.random((K, 3))
        #.dot  - multiplicação de matrizes
        colors = R.dot(random_colors)
        plt.scatter(X[:,0], X[:,1], c=colors)
        plt.show()

    print("Final cost", costs[-1])
    return M, R

#Função para transformar o setor em valor numérico, pois os algoritmos de Cluster trabalham apenas com dimensões numéricas
def transf_setor():
    tam = len(dfInvestVal)
    dfInvestVal2 = dfInvestVal
    n=0
    for n in range(tam):
        if dfInvestVal2['Setor'][n] == 'Comércio':
            dfInvestVal2['Setor'][n] = 1
        elif dfInvestVal2['Setor'][n] == 'Indústria':
            dfInvestVal2['Setor'][n] = 7
        elif dfInvestVal2['Setor'][n] == 'Infraestrutura':
            dfInvestVal2['Setor'][n] = 13
        elif dfInvestVal2['Setor'][n] == 'Serviços':
            dfInvestVal2['Setor'][n] = 19
        elif dfInvestVal2['Setor'][n] == 'Outros':
            dfInvestVal2['Setor'][n] = 25
        n += 1
    return dfInvestVal2

# Função para transformar o setor de cada item agregando valores aleatórios entre zero e um conforme distribuição normal através da função choice da biblioteca numpy
# essa função é na realidade mais para dar um efeito estético de cluster, apresentando os pontos como nuvem, senão ficaraim linhas/tripas
def transf_setor2():
    tam = len(dfInvestVal2)
    n=0
    fator = np.random.choice(3, tam)
    i = 0.0001
    for n in range(tam):
        if dfInvestVal2['Setor'][n] == 1:
            dfInvestVal2['Setor'][n] = (1) + fator[n]
        elif dfInvestVal2['Setor'][n] == 7:
            dfInvestVal2['Setor'][n] = (7) + fator[n]
        elif dfInvestVal2['Setor'][n] == 13:
            dfInvestVal2['Setor'][n] = (13) + fator[n]
        elif dfInvestVal2['Setor'][n] == 19:
            dfInvestVal2['Setor'][n] = (19) + fator[n]
        elif dfInvestVal2['Setor'][n] == 25:
            dfInvestVal2['Setor'][n] = (25) + fator[n]
        n += 1
    return dfInvestVal2

## Salvando o Nome do Setor e Tipo de Atividade em colunas relacionadas para preservar o texto original
dfInvestVal['SetorNome'] = dfInvestVal['Setor']
dfInvestVal['Tipo Investimento Nome']=dfInvestVal['Tipo Investimento']

# Para refazer o Setor - reexecute a partir daqui
dfInvestVal2=transf_setor()

dfInvestVal2['Setor'] = dfInvestVal2['Setor'].astype(int)

dfInvestVal2=transf_setor2()

#!pip install matplotlib
#!pip install sklearn.preprocessing
#!pip install -U scikit-learn
#!pip install -U matplotlib

dfInvestVal2.head()

"""## Criando Clusters com a lib skit learn - método K-Means"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler

"""# Standardize the data - Normalização/padronização dos Dados
Transformação dos dados - MinMaxScaler - fit_transform
https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/ Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.

The two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.

# Criando o Dataframe vet_x somente das colunas a serem clusterizadas
"""

## Criando o df somente das colunas a serem clusterizadas
vet_x = dfInvestVal2[['Setor','Real (em milhões)']]
print('vet_x -index mais as 2 dimensões a serem normalizadas para k-means', vet_x.head())

"""#  Padronização dos dados com MinMaxSacler"""

#  Padronização dos dados com MinMaxSacler
x_array = np.array(vet_x)
print('x_array - vetor de vet_x', x_array)
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform(x_array)
print('x_scaled - escalar normalizado de x_array para k-means',x_scaled)

"""# X = x_scaled - X é o escalar com MinMaxScaler"""

# Plotando os dados crus
X = x_scaled
print('Dados Padronizados com MinMaxScaler')
    # what does it look like without clustering?
plt.scatter(X[:,0], X[:,1])
plt.show()

X.shape

"""# Padronização com StandardScaler"""

## Padronização dos dados com StandardSacler
from sklearn.preprocessing import StandardScaler
x_array = np.array(vet_x)
print('x_array - vetor de vet_x', x_array)
scaler = StandardScaler()
x_scaled2 = scaler.fit_transform(x_array)
print('x_scaled2 - escalar normalizado com StandardScaler de x_array para DBSCAN',x_scaled2)

"""# X2 = x_scaled2 - X2 é o escalar com Standard Scaler"""

# Plotando os dados crus
X2 = x_scaled2
print('Dados Padronizados com Standard Scaler')
    # what does it look like without clustering?
plt.scatter(X2[:,0], X2[:,1])
plt.show()

def main2():
  #X = get_simple_data()

  plt.scatter(X[:,0], X[:,1])
  plt.show()

  costs = np.empty(10)
  costs[0] = None
  for k in range(1, 10):
    M, R = plot_k_means(X, k, show_plots=False)
    c = cost(X, R, M)
    costs[k] = c

  plt.plot(costs)
  plt.title("Cost vs K")
  plt.show()

"""# Elbow Method - Teste do Cotuvelo"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from warnings import filterwarnings
filterwarnings('ignore')
sum_of_squared_distances = []
k = range(1,15)
for i in k:
    km = KMeans(n_clusters = i)
    km = km.fit(x_scaled)
    sum_of_squared_distances.append(km.inertia_)
# ploting elbow
plt.plot (k, sum_of_squared_distances, 'bx-')
plt.xlabel ('k')
plt.ylabel ('sum_of_squared_distances')
plt.title('Elbow Method for Optimal K')
plt.show()

"""# Com o teste Elbow acima  iterando a funçãoinertia (mínimo da soma das distâncias) versus o número de clusters podemos compensar um dos pontos falhos do k-means que é exigir que definamos de ante-mão a quantidade de clusters. Pelo  gráfico acima temos que teríamos algo entre 4 e 6 clusters aliado à  intuição de negócio (dado que temos 5 setores) vamos escolher com 5 clusters"""

dfInvestVal2.head()

"""## Clustering com K-Means"""

kmeans = KMeans(n_clusters = 5, random_state=123)
#x_scaled - escalar normalizado de x_array para k-means
# A função fit é quem executa a clusterização
kmeans = kmeans.fit(x_scaled)
# Criando no dataframe completo a classificação/label do cluster à qual foi classificado
dfInvestVal2['klusterK2D'] = kmeans.labels_

fig=plt.figure (figsize=(9,7))

ax1=plt.subplot(1,1,1) # (1, 1, 1) = colum, row, and position

#ax1.bar(valueDate.index, valueDate.values) #bar graph

output = plt.scatter(x_scaled[:,0], x_scaled[:,1], s = 100, c = dfInvestVal2['klusterK2D'], marker = 'o', alpha = 1 )
centers = kmeans.cluster_centers_
plt.scatter(centers[:,0], centers[:,1], c='red', s=200, alpha=1 , marker='o');
plt.title('Valor x Setor  -Klustering K-Means')
plt.colorbar (output)
plt.show()

"""# Métricas do K-MEANS

Silhouette Coefficient Definition

The Silhouette Coefficient is calculated using the mean intra-cluster
distance (``a``) and the mean nearest-cluster distance (``b``) for each
sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,
b)``.  To clarify, ``b`` is the distance between a sample and the nearest
cluster that the sample is not a part of.
Note that Silhouette Coefficient is only defined if number of labels
is ``2 <= n_labels <= n_samples - 1``.

This function returns the mean Silhouette Coefficient over all samples.
To obtain the values for each sample, use :func:`silhouette_samples`.

The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters. Negative values generally indicate that a sample has
been assigned to the wrong cluster, as a different cluster is more similar.

# O coef de Silhueta ficou ótimo (Maior que 0.8). Ou seja tivemos 5 clusters bem formados, bem segregados
"""

from sklearn import metrics
from sklearn.metrics import silhouette_score

#x_scaled - VETOR NORMALIZADO
#kmeans = KMeans(n_clusters = 5, random_state=123)
klabels = kmeans.labels_

print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(x_scaled, klabels))

"""# Cluster Hierárquicos trouxeram um approach visual interessante, mas ofereceu poucas informações para análises mais aprofundadas como o k-means e o DBscan ofereceram.
# Temos a seguir duas aplicações de Cluster Hieráquicos - com a lib Scipy  através da função linkage (oferece 3 técnicas : Ward, simples e complete) e com a função multivariate_normal. Para ambas técnicas a visualização é feita com o gráfico Dendograma (gráfico que mostra hierarquia como num organograma).
# 1- HCluster - Hierarchical - com Scipy - com linkage e plot com dendograma
"""

# https://deeplearningcourses.com/c/cluster-analysis-unsupervised-machine-learning-python
# https://www.udemy.com/cluster-analysis-unsupervised-machine-learning-python
from __future__ import print_function, division
from future.utils import iteritems
from builtins import range, input
# Note: you may need to update your version of future
# sudo pip install -U future


import numpy as np
import matplotlib.pyplot as plt

from scipy.cluster.hierarchy import dendrogram, linkage

def main3():

    Z = linkage(X, 'ward')
    print("Z.shape:", Z.shape)
    # Z has the format [idx1, idx2, dist, sample_count]
    # therefore, its size will be (N-1, 4)

    # from documentation:
    # A (n-1) by 4 matrix Z is returned. At the i-th iteration,
    # clusters with indicesZ[i, 0] and Z[i, 1] are combined to
    # form cluster n + i. A cluster with an index less than n
    # corresponds to one of the original observations.
    # The distance between clusters Z[i, 0] and Z[i, 1] is given
    # by Z[i, 2]. The fourth value Z[i, 3] represents the number
    # of original observations in the newly formed cluster.
    plt.figure(figsize=(12,6))
    plt.title("Ward")
    dendrogram(Z)
    plt.show()

    plt.figure(figsize=(12,6))
    Z = linkage(X, 'single')
    plt.title("Single")
    dendrogram(Z)
    plt.show()

    plt.figure(figsize=(12,6))
    Z = linkage(X, 'complete')
    plt.title("Complete")
    dendrogram(Z)
    plt.show()

if __name__ == '__main__':
  main3()

"""# 2 - Hierarquical Clustering - HCluster com Scipy - multivariate_normal e plot com dendograma
Agglomerative Clustering

Recursively merges the pair of clusters that minimally increases
a given linkage distance - uses Ward method
"""

# https://deeplearningcourses.com/c/cluster-analysis-unsupervised-machine-learning-python
# https://www.udemy.com/cluster-analysis-unsupervised-machine-learning-python
from __future__ import print_function, division
from future.utils import iteritems
from builtins import range, input
# Note: you may need to update your version of future
# sudo pip install -U future


import numpy as np
import matplotlib.pyplot as plt

from scipy.stats import multivariate_normal

import numpy as np

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering


def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack([model.children_, model.distances_,
                                      counts]).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)


# setting distance_threshold=0 ensures we compute the full tree.
modelH = AgglomerativeClustering(distance_threshold=0, n_clusters=None)

modelH = modelH.fit(X)
plt.title('Hierarchical Clustering Dendrogram')
# plot the top three levels of the dendrogram
plot_dendrogram(modelH, truncate_mode='level', p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.figure(figsize=(12,6))
plt.show()

"""## Cluster with DBSCAN

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a very popular density based data clustering algorithm commonly used in data mining and machine learning. DBSCAN clusters the data points to separate the areas of high density with the areas of low density. It also marks data points as outliers that are in the low density regions. The clusters formed can be of varying shapes based on the density of data points.
"""

# Importing DBSCAN model for clustering
from sklearn.cluster import DBSCAN
import numpy as np
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

"""  Relembrando a transformação numérica que fizemos de Setor (DE/PARA)
  
  if dfInvestVal2['Setor'][n] == 'Comércio':
            dfInvestVal2['Setor'][n] = 1            
        elif dfInvestVal2['Setor'][n] == 'Indústria':
            dfInvestVal2['Setor'][n] = 7    
        elif dfInvestVal2['Setor'][n] == 'Infraestrutura':
            dfInvestVal2['Setor'][n] = 13    
        elif dfInvestVal2['Setor'][n] == 'Serviços':
            dfInvestVal2['Setor'][n] = 19    
        elif dfInvestVal2['Setor'][n] == 'Outros':
            dfInvestVal2['Setor'][n] = 25   
   
"""

X2.shape

np.arange(2,9)

"""## let’s take an iterative approach to fine-tuning our DBSCAN model."""

## let’s take an iterative approach to fine-tuning our DBSCAN model.
import itertools
pca_eps_values = np.arange(0.1,1.6,0.1)
pca_min_samples = np.arange(2,9)
pca_dbscan_params = list(itertools.product(pca_eps_values, pca_min_samples))
pca_dbscan_params
pca_no_of_clusters = []
pca_sil_score = []
pca_epsvalues = []
pca_min_samp = []
for p in pca_dbscan_params:
    pca_dbscan_cluster = DBSCAN(eps=p[0], min_samples=p[1]).fit(X2)
    pca_epsvalues.append(p[0])
    pca_min_samp.append(p[1])
    pca_no_of_clusters.append(len(np.unique(pca_dbscan_cluster.labels_)))
    pca_sil_score.append(silhouette_score(X2, pca_dbscan_cluster.labels_))
pca_eps_min = list(zip(pca_no_of_clusters, pca_sil_score, pca_epsvalues, pca_min_samp))
pca_eps_min_df = pd.DataFrame(pca_eps_min, columns=[['no_of_clusters', 'silhouette_score', 'epsilon_values', 'minimum_points']])
pca_eps_min_df

pca_eps_min_df.to_excel('DBSCAN-TUNING.xlsx')

# #############################################################################
# Compute DBSCAN
# Selecionamos os valores de eps e min_samples pegando o maior valor de silhueta para cluster =6 (no final fica cluster =5,
# pois 1 cluster concentra os outliers -noise) temo o eps = 0.3  e min_samples = 7
dbscan = DBSCAN(eps = 0.3, min_samples = 7)
modelDB2D = dbscan.fit(X2)
#db = DBSCAN(eps=0.3, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(modelDB2D.labels_, dtype=bool)
core_samples_mask[modelDB2D.core_sample_indices_] = True
labelsDB2D = modelDB2D.labels_

len(labelsDB2D)

dfInvestVal2['klusterDB2D'] = labelsDB2D

dfInvestVal2.head()

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labelsDB2D)) - (1 if -1 in labelsDB2D else 0)
n_noise_ = list(labelsDB2D).count(-1)

n_clusters_

np.unique(labelsDB2D, return_counts=True)

# Commented out IPython magic to ensure Python compatibility.
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
#print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labelsDB2D))
#print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labelsDB2D))
#print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labelsDB2D))
#print("Adjusted Rand Index: %0.3f"
#      % metrics.adjusted_rand_score(labels_true, labelsDB2D))
#print("Adjusted Mutual Information: %0.3f"
#      % metrics.adjusted_mutual_info_score(labels_true, labelsDB2D))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(X2, labelsDB2D))

"""# O coeficiente de Silhueta é o mais importante neste algoritmo. Quanto mais próximo de 1 melhor pois indica que os clusters estão perfeitamente definidos. A silhueta do k-means (0.8) ficou bem melhor que o DBScan nesse caso (0.72)"""

# #############################################################################
# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labelsDB2D)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labelsDB2D == k)

    xy = X2[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0],xy[:, 1] , 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

#    xy = X2[class_member_mask & ~core_samples_mask]
 #   plt.plot(xy[:, 1], xy[:, 0],'o', markerfacecolor=tuple(col),
 #            markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

"""# Refazendo os Cluster com 3 dimensões: Valor, Setor e Tipo Investimento"""

dfInvestVal2['Tipo Investimento'].unique()

#Função para transformar o 'Tipo Investimento' em valor numérico, pois os algoritmos de Cluster trabalham apenas com dimensões numéricas
def transf_tipoInvest():
    tam = len(dfInvestVal)
    dfInvestVal2 = dfInvestVal
    n=0
    for n in range(tam):
        if dfInvestVal2['Tipo Investimento'][n] == 'Implantação':
            dfInvestVal2['Tipo Investimento'][n] = 1
        elif dfInvestVal2['Tipo Investimento'][n] == 'Ampliação':
            dfInvestVal2['Tipo Investimento'][n] = 7
        elif dfInvestVal2['Tipo Investimento'][n] == 'Ampliação/Modernização':
            dfInvestVal2['Tipo Investimento'][n] = 13
        elif dfInvestVal2['Tipo Investimento'][n] == 'Modernização':
            dfInvestVal2['Tipo Investimento'][n] = 19
        n += 1
    return dfInvestVal2

# Função para transformar o setor de cada item agregando valores aleatórios entre zero e um conforme distribuição normal através da função choice da biblioteca numpy
# essa função é na realidade mais para dar um efeito estético de cluster, apresentando os pontos como nuvem, senão ficaraim linhas/tripas
def transf_tipoInvest2():
    tam = len(dfInvestVal2)
    n=0
    fator = np.random.choice(3, tam)
    i = 0.0001
    for n in range(tam):
        if dfInvestVal2['Tipo Investimento'][n] == 1:
            dfInvestVal2['Tipo Investimento'][n] = (1) + fator[n]
        elif dfInvestVal2['Tipo Investimento'][n] == 7:
            dfInvestVal2['Tipo Investimento'][n] = (7) + fator[n]
        elif dfInvestVal2['Tipo Investimento'][n] == 13:
            dfInvestVal2['Tipo Investimento'][n] = (13) + fator[n]
        elif dfInvestVal2['Tipo Investimento'][n] == 19:
            dfInvestVal2['Tipo Investimento'][n] = (19) + fator[n]
        n += 1
    return dfInvestVal2

dfInvestVal2=transf_tipoInvest()

dfInvestVal2['Tipo Investimento'] = dfInvestVal2['Tipo Investimento'].astype(int)

dfInvestVal2['Tipo Investimento'].unique()

dfInvestVal2=transf_tipoInvest2()

dfInvestVal2['Tipo Investimento'].unique()



"""# Criando a matriz vet_x2, agora com 3 dimensões - Setor, Tipo Investimento e Real (em milhoes)"""

len(dfInvestVal2[['Setor','Tipo Investimento','Real (em milhões)']])

vet_x2 = dfInvestVal2[['Setor','Tipo Investimento','Real (em milhões)']]
vet_x2.head()

x_array2 = np.array(vet_x2)
print(x_array2)

"""## Standardize the data - Base: x_array2
## Transformação dos dados - MinMaxScaler - fit_transform
https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/
Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.

This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.

The two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.
"""

scaler3D = MinMaxScaler()
x_scaled3D = scaler3D.fit_transform(x_array2)
print(x_scaled3D)
X3D = x_scaled3D
df_scaleMinMax = pd.DataFrame(x_scaled3D, columns=vet_x2.columns)
df_scaleMinMax.head()

#!pip install --upgrade matplotlib
df_scaleMinMax.head()

df_scaleMinMax.shape

# Import libraries
from mpl_toolkits import mplot3d
import numpy as np
import matplotlib.pyplot as plt

# Creating dataset
xMinMax = X3D[:,0]
yMinMax = X3D[:,1]
zMinMax = X3D[:,2]

"""# Standardize the data
# Transformação dos dados - StandardScaler - fit_transform
"""

scaler = StandardScaler()
scaler.fit(x_array2)
X_scale = scaler.transform(x_array2)
df_scaleStd = pd.DataFrame(X_scale, columns=vet_x2.columns)
# Creating dataset
xStd = X_scale[:,0]
yStd = X_scale[:,1]
zStd = X_scale[:,2]
df_scaleStd.head()





"""# Plotando os dados "crus" - transformados em 3D"""

from matplotlib import cm
# Creating figyre
fig = plt.figure(figsize =(14, 9))
ax = plt.axes(projection ='3d')

# Select the color map named rainbow
cmap = cm.get_cmap(name='rainbow')

# Creating plot
#ax.plot_surface(x, y, X2[:,2])
#ax.scatter(x, y, z,c = 'red')
#ax.plot(x, y, z)
#ax.plot_trisurf(x, y, z)
#plot_trisurf(x,y, triangles=triangles,z)
ax.plot_trisurf(xMinMax, yMinMax, zMinMax, linewidth=0.2, antialiased=True)

# Plot each vector with a different color from the colormap.
#for ind, (t, y, v) in enumerate(zip(timeDate, Y, V)):
#|    plt.vlines(t,y,v ,color = cmap(ind))

# show plot
plt.show()

# Gráfico com recurso de malha - mash

#Two additional examples of plotting surfaces with triangular mesh.

#The first demonstrates use of plot_trisurf's triangles argument, and the
#second sets a Triangulation object's mask and passes the object directly
#to plot_trisurf.
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.tri as mtri


fig = plt.figure(figsize=plt.figaspect(0.5))

#============
# First plot
#============

# Make a mesh in the space of parameterisation variables u and v
u = np.linspace(0, 2.0 * np.pi, endpoint=True, num=50)
v = np.linspace(-0.5, 0.5, endpoint=True, num=10)
u, v = np.meshgrid(u, v)
u, v = u.flatten(), v.flatten()

# This is the Mobius mapping, taking a u, v pair and returning an x, y, z
# triple

# Triangulate parameter space to determine the triangles
tri = mtri.Triangulation(u, v)

# Plot the surface.  The triangles in parameter space determine which x, y, z
# points are connected by an edge.
ax = fig.add_subplot(1, 2, 1, projection='3d')
ax.plot_trisurf(xMinMax, yMinMax, zMinMax, triangles=tri.triangles, cmap=plt.cm.Spectral)
ax.set_zlim(-1, 1)

"""# Plotting our raw data in a 3D space we can see some potential problems for Clustering"""

import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Tipo Investimento'),zaxis = dict(title  = 'Real (em milhões)'))
trace = go.Scatter3d(x=xMinMax, y=yMinMax, z=zMinMax, mode='markers',marker=dict(colorscale='Greys', opacity=0.3, size = 10, ))
#trace = go.Scatter3d(x=df_scale.iloc[:,0], y=df_scale.iloc[:,1], z=df_scale[:,2], mode='markers',marker=dict(colorscale='Greys', opacity=0.3, size = 10, ))
layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.show()

"""# Criando Clusters com a lib skit learn - método K-Means
https://towardsdatascience.com/explaining-k-means-clustering-5298dc47bad6

"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go
from plotly import tools
from plotly.subplots import make_subplots
import plotly.offline as py
import plotly.express as px
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
# %matplotlib inline
from warnings import filterwarnings
filterwarnings('ignore')

"""2. KMeans on Original Dataset
Let’s utilize the Elbow Method to determine the optimal number of clusters KMeans should obtain. It seem 4 or 5 clusters would be best and for the sake of simplicity we’ll select 5.
"""

df_scale3 = df_scaleMinMax.copy()
sse = []
k_list = range(1, 15)
for k in k_list:
    km = KMeans(n_clusters=k)
    km.fit(df_scale3)
    sse.append([k, km.inertia_])

oca_results_scale = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})
plt.figure(figsize=(12,6))
plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o')
plt.title('Optimal Number of Clusters using Elbow Method (Scaled Data)')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')

"""Veja que o k-means não parece ter bom desempenho para 3 dimensões, pois gerou apenas 4 cluster como número ótimo de clusters, o que claramente é muito aquém pela inspeção visual dos dados "crus". Mais abaixo temos a comprovação dessa ineficiência com o valor do teste de silhueta com apenas 0.53 (muito baixo se comparado com o valor da silhueta do DBSCAN que veremos abaixo com quase 0.9! Isto pelo k=means não houve uma boa segregação dos clusters."""

kmeans_scale = KMeans(n_clusters=5, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(df_scale3)
dfInvestVal2['klusterK3D'] = kmeans_scale.labels_
print('KMeans Scaled Silhouette Score: {}'.format(silhouette_score(df_scale3, kmeans_scale.labels_, metric='euclidean')))
labels_scale = kmeans_scale.labels_
clusters_scale = pd.concat([df_scale3, pd.DataFrame({'cluster_scaled':labels_scale})], axis=1)

kmeans_scale.labels_

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Tipo Investimento'),zaxis = dict(title  = 'Real (em milhões)'))
labels = labels_scale
trace = go.Scatter3d(x=xMinMax, y=yMinMax, z=zMinMax, mode='markers',marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))
layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.show()

"""#  Cluster 3D with DBSCAN
https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31
"""

#!pip install plotly

#Let’s Cluster!
import itertools
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as pyo
pyo.init_notebook_mode()
import plotly.graph_objs as go
from plotly import tools
from plotly.subplots import make_subplots
import plotly.offline as py
import plotly.express as px
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
plt.style.use('fivethirtyeight')
from warnings import filterwarnings
filterwarnings('ignore')

from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from collections import Counter

"""## DBSCAN Clustering
Approach #1
Before we apply the clustering algorithm we have to determine the appropriate epsilon level using the “Elbow Method” we discussed above. It would seem the optimal epsilon value is around 0.1. Finally, since we have 3 principal components to our data we’ll set our minimum points criteria to 6.
"""

plt.figure(figsize=(10,5))
nn = NearestNeighbors(n_neighbors=5).fit(df_scaleStd)
distances, idx = nn.kneighbors(df_scaleStd)
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)
plt.show()

pd.Series(labels).unique()

db = DBSCAN(eps=0.3, min_samples=4).fit(df_scaleStd)
labelsDB3D = db.labels_
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labelsDB3D)) - (1 if -1 in labelsDB3D else 0)
n_noise_ = list(labelsDB3D).count(-1)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(df_scaleStd, labelsDB3D))

dfInvestVal2.shape

len(labelsDB3D)

dfInvestVal2['klusterBD3D'] = labelsDB3D

dfInvestVal2.head()

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Tipo Investimento'),zaxis = dict(title  = 'Real (em milhões)'))
#labels = db.labels_
trace = go.Scatter3d(x=xStd, y=yStd, z=zStd, mode='markers',marker=dict(color = labelsDB3D, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))
layout = go.Layout(scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.update_layout(title='DBSCAN clusters (20) ', font=dict(size=12,))
fig.show()

np.unique(labelsDB3D, return_counts=True)

"""## Approach #2
Instead of using the “Elbow Method” and the minimum value heuristic let’s take an iterative approach to fine-tuning our DBSCAN model.
"""

df_scaleStd

pca_eps_values = np.arange(0.1,1.6,0.1)
pca_min_samples = np.arange(2,5)
pca_dbscan_params = list(itertools.product(pca_eps_values, pca_min_samples))
pca_dbscan_params
pca_no_of_clusters = []
pca_sil_score = []
pca_epsvalues = []
pca_min_samp = []
for p in pca_dbscan_params:
    pca_dbscan_cluster = DBSCAN(eps=p[0], min_samples=p[1]).fit(df_scaleStd)
    pca_epsvalues.append(p[0])
    pca_min_samp.append(p[1])
    pca_no_of_clusters.append(len(np.unique(pca_dbscan_cluster.labels_)))
    pca_sil_score.append(silhouette_score(df_scaleStd, pca_dbscan_cluster.labels_))
pca_eps_min = list(zip(pca_no_of_clusters, pca_sil_score, pca_epsvalues, pca_min_samp))
pca_eps_min_df = pd.DataFrame(pca_eps_min, columns=[['no_of_clusters', 'silhouette_score', 'epsilon_values', 'minimum_points']])
pca_eps_min_df

"""You might be asking yourself “weren’t we supposed to obtain 7 clusters?”. The answer is “Yes” and if we look at the unique labels/clusters we see 7 labels for each data point. Per Sklearn documentation, a label of “-1” equates to a “noisy” data point it hasn’t been clustered into any of the 6 high-density clusters. We naturally don’t want to consider any labels of “-1” as a cluster, therefore, they are removed from the calculation."""

db = DBSCAN(eps=0.3,min_samples=4).fit(df_scaleStd)
labelsDB3D2 = db.labels_
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labelsDB3D2)) - (1 if -1 in labelsDB3D2 else 0)
n_noise_ = list(labelsDB3D2).count(-1)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Silhouette Coefficient: %0.3f" % silhouette_score(df_scaleStd, labelsDB3D2))

# verificando a quantidade de elementos por cluster. o Cluster -1 são os outliers - desconsiderar. Ou seja abaixo temos 71 outliers
# e 0 cluster label = 3 sendo o maior com 1.1136 elementos, seguido pelo cluster zero com 797 elementos e o cluster 2 com 399 elementos.
#(array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,
#        16, 17, 18], dtype=int64),
# array([  71,  797,  259,  399, 1136,   17,  129,  284,   54,  294,  213,
#         120,   14,   58,  211,   28,   14,   63,   11,   15], dtype=int64))
np.unique(labelsDB3D2, return_counts=True)

Scene = dict(xaxis = dict(title  = 'Setor'),yaxis = dict(title  = 'Tipo Investimento'),zaxis = dict(title  = 'Real (em milhões)'))

# model.labels_ is nothing but the predicted clusters i.e y_clusters
#labels = db.labels_
trace = go.Scatter3d(x=xStd, y=yStd, z=zStd, mode='markers',marker=dict(color = labelsDB3D2, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))
layout = go.Layout(scene = Scene, height = 1000,width = 1000)
data = [trace]
fig = go.Figure(data = data, layout = layout)
fig.update_layout(title="'DBSCAN Clusters '", font=dict(size=12,))
fig.show()

# o DBSCAN têm uma certa dificuldade em identificar cluster quando tem diferentes densidades e/ou quando estão muito espalhados uns dos outros, que é o caso do maior cluster mais aéreo/ valores maiores de investimento no eixo Z

"""# Em uma análise macro para 2 dimensões o K-means teve melhor desempenho com silhoueta de 0.8 contra 0.72 do DBScan.Já para 3 clusterscom 3 dimensões o DBSCAN saiu melhor com uma silhoueta 0.61 x 0.58 do K-Means"""

dfInvestVal2.head()

dfInvestVal2['klusterK2D'].unique()

dfInvestVal2['klusterK3D'].unique()

dfInvestVal2['klusterDB2D'].unique()

dfInvestVal2['klusterBD3D'].unique()

dfInvestVal2.to_csv('dfClustersFinal.csv')



